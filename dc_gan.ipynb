{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISLP7e8o6ZDK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# importing Neccessary Library and constant variable\n",
    "\n",
    "# !pip install tf_clahe\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from packaging import version\n",
    "import os\n",
    "from packaging import version\n",
    "from datetime import datetime\n",
    "# Import writer class from csv module\n",
    "from csv import DictWriter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "IMG_H = 256\n",
    "IMG_W = 256\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "\n",
    "print(\"TensorFlow version: \", tf.keras.__version__)\n",
    "assert version.parse(tf.keras.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "# Weight initializers for the Generator network\n",
    "WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=IMG_C)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "#     rescailing image from 0,255 to -1,1\n",
    "    img = (img - 127.5) / 127.5\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def tf_dataset(images_path, batch_size, labels=False, class_names=None):\n",
    "  \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
    "    dataset = dataset.shuffle(buffer_size=10240)\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nZU71ZylN0y"
   },
   "outputs": [],
   "source": [
    "# load image dataset for trainnig without labels\n",
    "def load_image_train(filename, batch_size):\n",
    "\t# load image with the preferred size\n",
    "    \n",
    "    pixels = tf_dataset(filename, batch_size)\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFL24bEX65GT"
   },
   "outputs": [],
   "source": [
    "# we'll use cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # First argument of loss is real labels\n",
    "    # We've labeled our images as 1 (real) because\n",
    "    # we're trying to fool discriminator\n",
    "    return cross_entropy(tf.ones_like(fake_output),fake_output)\n",
    "\n",
    "gen_optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "def discriminator_loss(real_images,fake_images):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_images),real_images)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_images),fake_images)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator model based on resnet50 and unet network\n",
    "def build_generator(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Random noise to 16x16x256 image\n",
    "    model.add(tf.keras.layers.Dense(16*16*256,use_bias=False,input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Reshape((16,16,256)))\n",
    "    \n",
    "    assert model.output_shape == (None,16,16,256)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    assert model.output_shape == (None,32,32,128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    assert model.output_shape == (None,64,64,128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    assert model.output_shape == (None,128,128,64)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(3,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT,\n",
    "                                     activation=\"tanh\"\n",
    "                                    ))\n",
    "              # Tanh activation function compress values between -1 and 1. \n",
    "              # This is why we compressed our images between -1 and 1 in readImage function.\n",
    "    assert model.output_shape == (None,256,256,3)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discriminator model\n",
    "def build_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\", input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128,(5,5),strides=(2,2),padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(265,(5,5),strides=(2,2),padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm_XokrmFnlN"
   },
   "outputs": [],
   "source": [
    "def save_plot(examples, name_model, n):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n",
    "    filename = f\"samples/generated_plot-{name_model}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "class DCGAN(tf.keras.models.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim, batch_size):\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "        # Regularization Rate for each loss function\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-6, beta_1=0.5, beta_2=0.999)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-6, beta_1=0.5, beta_2=0.999)\n",
    "    \n",
    "    \n",
    "    def compile(self, g_optimizer, d_optimizer, filepath, loss_fn, resume=False):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "            \n",
    "# Notice the use of `tf.keras.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "    @tf.function\n",
    "    def train_step(self, images):\n",
    "        # We've created random seeds\n",
    "        noise = tf.random.normal([self.batch_size, self.latent_dim])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generator generated images\n",
    "            generated_images = self.generator(noise,training=True)\n",
    "\n",
    "            # We've sent our real and fake images to the discriminator\n",
    "            # and taken the decisions of it.\n",
    "            real_output = self.discriminator(images,training=True)\n",
    "            fake_output = self.discriminator(generated_images,training=True)\n",
    "\n",
    "            # We've computed losses of generator and discriminator\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output,fake_output)\n",
    "\n",
    "        # We've computed gradients of networks and updated variables using those gradients.\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": gen_loss,\n",
    "            \"disc_loss\": disc_loss\n",
    "        }\n",
    "\n",
    "\n",
    "    def saved_model(self, gmodelpath, dmodelpath):\n",
    "        self.generator.save(gmodelpath)\n",
    "        self.discriminator.save(dmodelpath)\n",
    "\n",
    "    def loaded_model(self, g_filepath, d_filepath):\n",
    "        self.generator.load_weights(g_filepath)\n",
    "        self.discriminator.load_weights(d_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 g_model_path,\n",
    "                 d_model_path,\n",
    "                 logs_file,\n",
    "                 name_model\n",
    "                ):\n",
    "        super(CustomSaver, self).__init__()\n",
    "        self.g_model_path = g_model_path\n",
    "        self.d_model_path = d_model_path\n",
    "        self.logs_file = logs_file\n",
    "        self.name_model = name_model\n",
    "        self.epochs_list = []\n",
    "        self.gen_loss_list = []\n",
    "        self.disc_loss_list = []\n",
    "        \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not hasattr(self, 'epoch'):\n",
    "            self.epoch = []\n",
    "            self.history = {}\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.saved_model(self.g_model_path, self.d_model_path)\n",
    "        \n",
    "        self.plot_epoch_result(self.epochs_list, self.gen_loss_list, \"Generator_Loss\", self.name_model, \"g\")\n",
    "        self.plot_epoch_result(self.epochs_list, self.disc_loss_list, \"Discriminator_Loss\", self.name_model, \"r\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "#             print(k, v)\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        self.epochs_list.append(epoch)\n",
    "        self.gen_loss_list.append(logs[\"gen_loss\"])\n",
    "        self.disc_loss_list.append(logs[\"disc_loss\"])\n",
    "\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % 15 == 0 or (epoch + 1) <= 15:\n",
    "            self.model.saved_model(self.g_model_path, self.d_model_path)\n",
    "            print('saved for epoch',epoch + 1)\n",
    "            \n",
    "    def plot_epoch_result(self, epochs, loss, name, model_name, colour):\n",
    "        plt.plot(epochs, loss, colour, label=name)\n",
    "    #     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "        plt.title(name)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(model_name+ '_'+name+'_epoch_result.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 1500:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.keras.math.exp(-0.1)\n",
    "\n",
    "def set_callbacks(name_model, logs_path, logs_file, path_gmodal, path_dmodal, steps):\n",
    "    # create and use callback:\n",
    "    \n",
    "    saver_callback = CustomSaver(\n",
    "        path_gmodal,\n",
    "        path_dmodal,\n",
    "        logs_file,\n",
    "        name_model\n",
    "    )\n",
    "    \n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='disc_loss', factor=0.2,\n",
    "                              patience=7, min_lr=0.000001)\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=logs_path + name_model + \"/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "        histogram_freq=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    callbacks = [\n",
    "        saver_callback,\n",
    "#         checkpoints_callback,\n",
    "        tensorboard_callback,\n",
    "#         lr_callback,\n",
    "        reduce_lr,\n",
    "    ]\n",
    "    return callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trainning(model, train_dataset,num_epochs, path_gmodal, path_dmodal, logs_path, logs_file, name_model, steps, resume=False):\n",
    "\n",
    "    \n",
    "    \n",
    "    callbacks = set_callbacks(name_model, logs_path, logs_file, path_gmodal, path_dmodal, steps)\n",
    "            \n",
    "    model.fit(train_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "    \n",
    "def testing(model, g_filepath, latent_dim , name_model, n_samples=25):\n",
    "    noise = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "    # g_model = model.load(g_filepath)\n",
    "    g_model = tf.keras.models.load_model(g_filepath)\n",
    "\n",
    "    examples = g_model.predict(noise)\n",
    "    save_plot(examples, name_model, int(np.sqrt(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KoSI9-4-tVt"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    '''\n",
    "    In Default:\n",
    "    Clahe: OFF\n",
    "    BCET: OFF\n",
    "    Resize: crop or padding (decided by tensorflow)\n",
    "    Datasets: For trainning dataset, it'll have additional datasets (flip-up-down and flip-right-left)\n",
    "    '''\n",
    "    \n",
    "    # run the function here\n",
    "    \"\"\" Set Hyperparameters \"\"\"\n",
    "    \n",
    "    batch_size = 128\n",
    "    num_epochs = 150\n",
    "    latent_dim = 100\n",
    "    name_model= str(IMG_H)+\"_dc_gan_\"+str(num_epochs)\n",
    "    \n",
    "    resume_trainning = False\n",
    "    lr = 1e-5\n",
    "    \n",
    "    print(\"start: \", name_model)\n",
    "    \n",
    "    # set dir of files\n",
    "    train_images_path = \"data_test/*.jpg\"\n",
    "    saved_model_path = \"saved_model/\"\n",
    "    \n",
    "    logs_path = \"logs/\"\n",
    "    \n",
    "    logs_file = logs_path + \"logs_\" + name_model + \".csv\"\n",
    "    \n",
    "    path_gmodal = saved_model_path + name_model + \"_g_model\" + \".h5\"\n",
    "    path_dmodal = saved_model_path +  name_model + \"_d_model\" + \".h5\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a MirroredStrategy object. \n",
    "    This will handle distribution and provide a context manager (MirroredStrategy.scope) \n",
    "    to build your model inside.\n",
    "    \"\"\"\n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    input_shape = (IMG_H, IMG_W, IMG_C)\n",
    "    # print(input_shape)\n",
    "    \n",
    "    ## init models ##\n",
    "    \n",
    "    d_model = build_discriminator(input_shape)\n",
    "    g_model = build_generator((latent_dim, ))\n",
    "\n",
    "    \n",
    "#     d_model.summary()\n",
    "#     g_model.summary()\n",
    "    \n",
    "    resunetgan = DCGAN(g_model, d_model, latent_dim, batch_size)\n",
    "    \n",
    "    bce_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999)\n",
    "    \n",
    "    resunetgan.compile(g_optimizer, d_optimizer, logs_file, bce_loss_fn, resume_trainning)\n",
    "    \n",
    "    \"\"\" run trainning process \"\"\"\n",
    "    train_images = glob(train_images_path)\n",
    "    train_images_dataset = load_image_train(train_images, batch_size)\n",
    "    train_images_dataset = train_images_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    size_of_dataset = len(list(train_images_dataset)) * batch_size\n",
    "    \n",
    "    steps = int(size_of_dataset/batch_size)\n",
    "    run_trainning(resunetgan, train_images_dataset, num_epochs, path_gmodal, path_dmodal, logs_path, logs_file, name_model, steps,resume=resume_trainning)\n",
    "    \n",
    "    testing(g_model, path_gmodal, latent_dim, name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mura_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
